{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "We will use neural networks as a running example of a data model in this course. We need some concrete tangible models in our discussions, I feel it is sensible to adopt deep neural networks, because\n",
    "- Currently (2019) they provide superior prediction performance in a wide range of practical applications. Arguably, deep neural network model champions in the widest range of problems among any single data model family does. \n",
    "- The tools of building, testing and deploying state-of-the-art neural networks are increasingly mature nowadays. Online learning materials and video tutorials/lectures are abundant.\n",
    "- The design principle is elegant (though practical implementations have not come close to the ultimate goal of end-to-end learning), which saves much ad hoc data engineering effort.\n",
    "\n",
    "\n",
    "Nonetheless, it is worth noting that the learning principles we study in this course apply to generic data models, not limited to deep neural networks. For techniques closely coupled with DNN, e.g. skip-layer-connections (see below), we will either introduce them in this lecture or specify explicitly when encountering them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recursive Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Review of Linear Model__ Linear model -- allocate each $X$-variable a _weight_ (i.e. coefficient, we will use \"weight\" following convention in the context of NN), then consider the (or every) desired $Y$-variable to be a simple function of the weighted sum. \n",
    "$$\n",
    "Y = \\psi(\\sum_i w_i X_i)\n",
    "$$\n",
    "If you have multiple $Y$-variables:\n",
    "\n",
    "$$\n",
    "Y_1 = \\psi(\\sum_i w_{1,i} X_i) \\\\\n",
    "Y_2 = \\psi(\\sum_i w_{2,i} X_i) \\\\\n",
    "\\dots\n",
    "$$\n",
    "which is\n",
    "$$\n",
    "Y_j = \\psi(\\sum_i w_{j,i} X_i)\n",
    "$$\n",
    "\n",
    "Recall our figure illustrating the computation:\n",
    "<img src=\"ref/illu-linear.png\" width=\"500px\"/>\n",
    "\n",
    "The figure below shows an example of a linear model predicting 2 $Y$-variables (recall the discussion in our previous class)\n",
    "<img src=\"ref/illu-linear-2y.png\" width=\"400px\"/>\n",
    "where each group of computations can be seen as an independent linear model for its respective target $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive design: bottom-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly, neural networks are multiple linear models grouped and stacked together. Let us first re-draw some elements and links in the figure above. There is a linear model of two prediction targets. We link the prediction computation for both targets to the data attributes, which makes the figure a bit messy but putting more focus on the important aspects for the construction of neural networks.\n",
    "\n",
    "<img src=\"ref/illu-linear-2ya.png\" width=\"450px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Straightforwardly, we can extend the model to generate more targets, say, $q$. We draw weight in groups as $w_{1,\\cdot}$, for all weights associated with the prediction of $y_1$.\n",
    "<img src=\"ref/mlp-botup-1.png\" width=\"450px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a very natural extension to consider is what if \n",
    "> instead of directly take the $y_1, y_2, \\dots$ as  the model prediction of the targets, we use them as intermediate features, and take further steps to construct $z_1, z_2, \\dots$ as the prediction goal.\n",
    "\n",
    "The figure below shows the extension. Of course, we need to introduce new sets of model parameters (weights) for the extension, see $u_{1,\\cdot}, u_{2,\\cdot}, \\dots$.\n",
    "\n",
    "<img src=\"ref/mlp-botup-2.png\" width=\"450px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive design: top-down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of viewing the construction of a neural network model also starts with a linear model. This time we are motivated by the idea of making the data attributes more representative.  So instead of using the raw observable variables, we compose the attributes using \"feature-engineering\" models, which are also linear (so the same idea applies recursively). The figure below shows the scheme.\n",
    "\n",
    "<img src=\"ref/mlp-td.png\" width=\"550px\"/>\n",
    "\n",
    "The figure shows how attribute-2 is replaced by the output of another \"lower-level\" model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a modern framework \"PyTorch\", we can easily implement a neural network model. Mainly, we are concerned with two aspects\n",
    "1. __definition__: to specify the structure of the model: number of layers, number of output units (variables) in each layer, how the inputs are related to the outputs (we only encountered fully connected layers for now, i.e. each input is connected to each output). We specify the definition in the `__init__`  (constructor) method of a neural network class.\n",
    "2. __computation__: the `forward` method performs the actual computations using the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">OPTIONAL</span>\n",
    "> One issue might be of interest for our careful readers: the _activation_ (non-linear map $\\psi$) function conceptually belongs to the model architecture. However, as there is no __adjustable__ parameters in those maps, they need no learning. Colloquially,  one can just consider they are too simple to be included in the model definition. In a more theoretical view, the parameter-less functions have no effect on where the model is in $\\mathcal H$, i.e. the current $h$. However, they do affect $\\mathcal H$ itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The computations in a 3 layer neural network. \n",
    "    \n",
    "    SEE ALSO the \"MLPClassifier\" we had used in `sklearn.neural_network`. \n",
    "    Here we will study the inner structure of a neural network.\n",
    "    \n",
    "    This construction is slightly more complex than using the simple interface\n",
    "    of MLPClassifier, but provides much more flexibility.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__() # Important: register self as an NN model\n",
    "        self.linear1 = nn.Linear(in_features=4, out_features=16)\n",
    "        self.linear2 = nn.Linear(in_features=16, out_features=8)\n",
    "        self.linear3 = nn.Linear(in_features=8, out_features=3)\n",
    "        \n",
    "        # out-features of i-1 must be the same as in-features of i\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # where the actual computation takes place\n",
    "        h1 = nn.functional.relu(self.linear1(x))\n",
    "        # \"relu\" is the elementwise map.\n",
    "        h2 = nn.functional.relu(self.linear2(h1))\n",
    "        h3 = nn.functional.relu(self.linear3(h2))\n",
    "        \n",
    "        # let's consider h3 be the \"logits\" of the classes\n",
    "        return h3\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note `relu` is a kind of elementwise \"activation\", i.e. nonlinear transformation ($\\psi$ in the figures above). Let's do a sanity test first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = torch.rand(10, 4) # allocate dummy input for the data model\n",
    "nn_model = MyNetwork() # create an instance of the model\n",
    "pred = nn_model(dummy_data) # when used like a function, the model's \"forward\" method is executed\n",
    "print(\"NN transforms data of {}, obtain {}\".format(\n",
    "    dummy_data.shape, pred.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">__Discussion__</span>\n",
    "- Modify the code above to let the model output diagnostic information during the `forward` computaiton, about the `shape` of each intermediate result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-propagation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-based optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the discussion about NN models in the last lecture. We can easily perturb the parameters of an NN model a bit to see if the model behaves better,  taking the advantage of the fact that as long as we keep the _mathematical form_ of the model parameters intact, we always have a valid neural network. In other words, we can easily \"move around\" in the _space of neural networks_.\n",
    "\n",
    "The main question is: where to move? A reasonable argument for an efficient move is to achieve the maximal change of a numerical criterion given fixed step size, say 1.0. More specifically, assume for now we know the following facts:\n",
    "\n",
    "- If the parameter $w_1$ changes by $\\Delta$, the criterion would change $3.0 \\Delta$ in response. \n",
    "- If the parameter $w_2$ changes by $\\Delta$, the criterion would change $4.0 \\Delta$ in response. \n",
    "\n",
    "Then for a fixed step size 1.0 for the combined adjustment in the 2D space consisting of $w_1$ and $w_2$, the most efficient movement is to let $\\Delta w_1 = 0.6$ and $\\Delta w_2=0.8$ (or the opposite direction, if the aim is to decrease the criterion).\n",
    "\n",
    "<span style=\"color:blue\">__Discussion__</span>\n",
    "Why that is the optimal movement direction? Could you try to come up with something else?\n",
    "\n",
    "\n",
    "In mathematical terms, let us put together all model parameters (i.e. weights and bias terms in ALL layers) and denote as $\\boldsymbol \\theta$, and denote the _parameter space_ consisting of all possible $\\boldsymbol \\theta$ as $\\boldsymbol \\Theta$. Naturally, starting from an existing neural network $\\boldsymbol \\theta$, we want to search for a promising movement in $\\boldsymbol \\Theta$. The figure below shows the process.\n",
    "\n",
    "<img src=\"ref/bp0.png\" width=\"500px\"/>\n",
    "\n",
    "The process is better known as \"optimisation\". For each individual parameter, we carefully calculate its effect on the model's fitness to observed data. The parameter-fitness relation helps us determine the apparently \"promising\" direction to move the model in $\\boldsymbol \\Theta$. The direction is the _gradient _ of fitness with respect to the model parameters.\n",
    "\n",
    "Fortunately, people had found smart and fast way to compute the gradient for a neural network, despite the fact that a parameter may contribute to the final model output (and thus affect its fitness to data) in a very tortuous route -- consider one weight associated with the input-output in the bottom layer in a 10 layer model.\n",
    "\n",
    "Before we study how to compute neural network gradients, let's first heed the limitation of such a method as the means of model optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient-based methods only suit some model families. At least, the family should register with some parameter space, and in the space, each point represents a valid member model of the family, as the $\\boldsymbol \\Theta$ in the neural networks' case. As a counter example, consider a decision tree model, it is not trivial how to search \"locally\" around an existing decision tree, as certain parameters of a model, e.g. the number of nodes and the structure, cannot be modified continuously. \n",
    "- Gradient is a __local__ matter. The direction of adjusting the model remains good only when the adjustment (step-size) is small.\n",
    "- The _loss_, i.e. \"the model's fitness to the observed data\", needs to be further scrutinised. i) \"Observed data\" means the gradient is calculated to improve the __estimated__ performance, which is closely related to the training-test generalisation issue. ii) In general, the complete \"observed data\" is a large object and can be cumbersome to manipulate in a computer's memory. So each step of adjustment is calculated using a small _batch_ of data. The batch represents a even smaller portion of data. We can only _expect_ the cumulated adjustments to lead to a good model. This is often referred to as \"stochastic gradient descend\" (SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradients via Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principle of calculating the effect on the loss for individual parameters is to skillfully formulate the standard chain rule for computing the derivations. The figure below shows this principle of computation\n",
    "\n",
    "<img src=\"ref/bp.png\" width=\"700px\"/>\n",
    "\n",
    "The \"EMSG\" stands for \"error message\", the desired change in the _result_ of some computation. EMSG is the information we \"back-propagate\" through the computational graph. Using EMSG, we compute the \"UMSG\" (update-message), i.e. how to change the operands in a computation to achieve the desired change. UMSG represents the gradient with respect to the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Implementation__\n",
    "\n",
    "Though conceptually simple, the implementation of the bp-algorithm needs painstaking care and is error-prone. Fortunately, mature computation frameworks have been developed and hide much of the details from model builders nowadays.\n",
    "\n",
    "In \"PyTorch\", most operations have a two-way implementation of the computation involved -- the forward computation and the backward calculation of the gradients with respect to the operands. Let us compare the \"conventional\" computation (forward only) and the two-way computation using a simple example.\n",
    "\n",
    "E.g. the operator \"+\" and \"*\" in the following computations are trivially straightforward, and work as expected. \n",
    "```python\n",
    "a = 5 + 3\n",
    "b = a * 2\n",
    "```\n",
    "On the other hand, if we let pytorch handle some operands,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([5.0]).requires_grad_()\n",
    "b = (a + 3.0) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write `[5.0]` instead of `5.0` because torch (and most other similar libraries) is designed to handle number arrays, in their mangaged data type, \"Tensor\". To construct a tensor, we need a **collection** of data. So we make a $[1 \\times 1]$ array to represent a real number. The qualification `requires_grad_()` is to explicitly state that `pytorch` should handle the backward computation -- which is _unnecessary_ in constructing actual learnable models.\n",
    "\n",
    "Let's check the effect of the backward computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a = {}, grad is {}\".format(a.data, a.grad))\n",
    "b.backward()\n",
    "print(\"a = {}, grad is {}\".format(a.data, a.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of `a.grad = [2.0]` means if the `a` changes by 1 unit, `b` will change 2 units in response.\n",
    "\n",
    "When using the framework to perform computations stated in a deep neural network model, the backward computation will result in the gradient, the direction toward which to modify the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks have a long history in the development of machine intelligence and statistical learning. Though the neural network model is versatile and in theory can express arbitrary relationships, the naive implementation and application usually lead to poor results, for both analytics and prediction tasks. They slowly gained popularity in many of the application areas only after appropriate techniques or model architectures suitable to the applications had matured. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter sharing (spatial): convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important limitation of the neural network models is the number of _adjustable_ parameters that are involved in all the steps of computations. Consider one particular step, where there are $m$ input variables and $n$ output ones. Since each output variable needs a weighted sum of __all__ the input variables, which entails $n$ weights. \n",
    "\n",
    "Consider the following example of dealing with data of pixels in pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining hand-written digits images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ignore the operations for now, we will introduce techniques of dealing with data storage, loading and preprocessing in a later class.\n",
    "\n",
    "The deliverable of this section is a data loader, when asked, it will yield an (x, y) pair, where x is a batch of images and y the corresponding digits.By \"ask\", I mean to iterate through the data, using loop operations such as \n",
    "```python\n",
    "for x, y in data_loader\n",
    "    ...\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "# where to store the data\n",
    "MY_DATA_DIR = \"../data\"\n",
    "\n",
    "DATA_TRANSFORM = transforms.ToTensor()\n",
    "\n",
    "dataset = MNIST(MY_DATA_DIR, train=True, \n",
    "                download=True, # if you don't have it already, download\n",
    "                transform=DATA_TRANSFORM)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in data_loader:\n",
    "    break # stop after having the first loading\n",
    "    \n",
    "print(\"X has a shape of\", x.shape, \"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualise one data sample, you can activate the code below\n",
    "%matplotlib inline\n",
    "if False:\n",
    "    import matplotlib.pyplot as plt\n",
    "    i = 0 # you can try to look at different samples.\n",
    "    plt.imshow(x[i, 0].numpy(), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivating and constructing convolutional networks\n",
    "Now we have 4 samples in an `x`. Consider one sample for now. It has $28 \\times 28=784$ variables (which could be multiplied by 3 for images have 3 channels for RGB colours). See the figure below.\n",
    "\n",
    "<img src=\"ref/hw1.png\" width=\"128\"/>\n",
    "\n",
    "To build a linear layer for the image data, each output variable of the layer needs 784 weights to associate with the pixel values (ignore bias for now). If the layer has $n$ output variables, $784n$ weights are needed to build the layer.\n",
    "\n",
    "Moreover, to serve as an intermediate processing step, which passes information of the data to upstream steps, $n$ cannot be too small compared to the original size $784$. So the parameter number in one layer is at the magnitude of $784^2 ~= 600k$. A network of a dozen layers can easily contain 5~10M parameters -- and keep in mind that the data of question are $28 \\times 28$ gray images! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be concrete let's make a network just for fun:\n",
    "\n",
    "class FullNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=784, out_features=1024)\n",
    "        self.linear2 = nn.Linear(in_features=1024, out_features=1024)\n",
    "        self.linear3 = nn.Linear(in_features=1024, out_features=1024)\n",
    "        self.linear4 = nn.Linear(in_features=1024, out_features=10)\n",
    "        # 10 for 10 different classes as the target of the analysis\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h = x.view(batch_size, -1) # [m, 1, 28, 28] -> [m, 784] \n",
    "        # \"flatten\" all pixels to process\n",
    "        h = nn.functional.relu(self.linear1(h))\n",
    "        h = nn.functional.relu(self.linear2(h))\n",
    "        h = nn.functional.relu(self.linear3(h))\n",
    "        h = nn.functional.log_softmax(self.linear4(h), dim=-1)\n",
    "        return h\n",
    "        \n",
    "        \n",
    "fnet_model = FullNetwork()\n",
    "h = fnet_model(x) \n",
    "print(\"10 class likelihood for each image\", h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We roughly count the size of the network by saving the model to disk and checking the file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let count the size of the network\n",
    "torch.save(fnet_model.state_dict(), \"../data/fullnn.pth\")\n",
    "\n",
    "# I got ~ 11 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of a convolutional neural network is both intuitive and intriguing. Let us take a look at a sample of our raw data, i.e. an image, for the inspiration.\n",
    "\n",
    "<img src=\"ref/imagedata.png\" width=\"450px\">\n",
    "\n",
    "The above figure illustrates the computation in a neural network highlighting the fact that the input data is an image and the variables are the pixels. (For less clutter, we focus on the connections and ignoring the sum and map ($\\phi$) operations). One characteristic of data in image format is the spatial structure of the input variables. The idea goes like: if some output unit is good at representing a meaningful visual feature, it would NOT care where the feature appears. So we may apply the same set of weights and let it run over the image plane.\n",
    "\n",
    "<img src=\"ref/conv.png\" width=\"450px\">\n",
    "\n",
    "\n",
    "\n",
    "In this way, we reuse a small number of weights to compute a large group of output variables. E.g. we need only to allocate $3\\times3=9$ weights to compute one output variable scanning each $3\\times3$ area over the image plane. For a $28\\times28$ image, there are $26\\times26$ valid positions to apply the computation, which results in 676 outputs using only 9 weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">__Disucssion__</span>\n",
    "Why $26 \\times 26$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saving in model parameters comes with a cost that the output variables are heavily dependent on each other. Thus we employ multiple groups of output variables in each convolution step. The groups are called \"channels\", suggesting an analogy to the colour channels in the original image.\n",
    "\n",
    "Fortunately, modern deep neural network frameworks provide all the operations involved in a convolution computation, including the weighted sum, scanning over image plane and the (usually burdensome and error-prone) backward propagation algorithm. Below is an implementation of a simple network in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be concrete let's make a 10-layer network just for fun:\n",
    "class ConvNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.linear4 = nn.Linear(in_features=32*3*3, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h = nn.functional.relu(nn.functional.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        h = nn.functional.relu(nn.functional.max_pool2d(self.conv2(h), kernel_size=2, stride=2))\n",
    "        h = nn.functional.relu(self.conv3(h))\n",
    "        h = h.view(batch_size, -1)\n",
    "        h = nn.functional.log_softmax(self.linear4(h), dim=-1)\n",
    "        return h\n",
    "        \n",
    "        \n",
    "cv_model = ConvNetwork()\n",
    "h = cv_model(x) \n",
    "print(\"10 class likelihood for each image\", h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let count the size of the network\n",
    "torch.save(cv_model.state_dict(), \"../data/cnn.pth\")\n",
    "# I got ~ 70 KB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter sharing (temporal): Recursive neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural networks (or any data model) we had encountered so far takes an independent view of the processing of the data samples. The processing of a sample $x_5$ has nothing to do with the processing of the preceding ones $x_4, x_3, \\dots$. The model is stateless. If you are familiar with how web protocols work, those models work like the HTTP protocol. You can imagine the model as an HTTP server, handling stateless connections. When a client sends data to process, the server processes the data and allows the client to take the results. After one session, the processing server completely forgets the client and its data. \n",
    "\n",
    "Such a computation framework may work well for tasks such as recognising an image as one object class or another. However, there are practical tasks requires the data model to have a memory, and investigate temporal relations in the data. For example, if the task is to recognise a football players strategy in the pitch, it would be necessary to take a video clip and examine the frame images containing the player for a period. Another example would be in natural language processing, the meaning of one word must be put in the context for appropriate understanding. Using our analogy above, some tasks need to add the functions of \"log-in\" or \"cookies\"  to the vanilla HTTP server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution is introduce _recursive_ connections in a network architecture. To be concrete, let's check the following modification of our old network:\n",
    "\n",
    "<img src=\"ref/rnn1.png\" width=\"200px\">\n",
    "\n",
    "If you try to program in your mind how to compute this network model and find the red links appear weird, you have got the point of those \"recursive\" links. The red links connect the outputs of processing one sample  $x$ to the processing of the next $x'$.\n",
    "\n",
    "To clarify the idea, let us consider an even simpler model as shown below\n",
    "\n",
    "<img src=\"ref/rnn2.png\" width=\"220px\">\n",
    "\n",
    "Some example computation steps performed by the network are\n",
    "4. $y_1^{(4)} \\leftarrow \\dots $\n",
    "5. $y_1^{(5)} \\leftarrow \\psi(w_{1,1} \\cdot x_1^{(5)} + u_{1,1} \\cdot y_1^{(4)})$\n",
    "6. $y_1^{(6)} \\leftarrow \\psi(w_{1,1} \\cdot x_1^{(6)} + u_{1,1} \\cdot y_1^{(6)})$\n",
    "7. ...\n",
    "\n",
    "__NB__: Do not mistake the term \"recursive\" here with the \"recursive design principle\" of generic neural networks, which we had discussed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation through time and practical solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One immediate question about those recursive neural networks (RNN) is how to determine the parameters associated with the temporal connections (red ones in figures above), which links $y^{(t-1)}$ to $y^{(t)}$. Remind the gradient-based approach we had learned above, the key is to compute how a small change of a weight, say, $u_{1,1}$ in the figure above, affects the final criterion. The final criterion, e.g. the classification errors or prediction accuracy, involves either the output $y_1$ at all times, or that of the ultimate stage. Therefore, to appreciate the main challenge, please consider the contribution of $u_{1,1}$ to some $y_1^{(10)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ref/rnn3.png\" width=\"240px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure partially illustrates how the weight of interest $u_{1,1}$ is involved in the outcome of $y_1^{10}$. The effect is multifold: $u_{1,1}$ affects the $y$-variable at an early moment $y_1^{(9)}$ which contributes to $y_1^{(10)}$. But moreover, $y_1^{(9)}$ itself is affected by $u_{1,1}$, through its multiplication with an earlier $y_1^{(8)}$. To account for the multifold effect of a parameter in an RNN, we need to apply the chain rule of computing the derivation through all the steps. This is also implemented using the backpropagation algorithm, which is referred to as \"backpropagation through time\" (BPTT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though theoretically sound, BPTT is unstable in practice. The vanilla version of the algorithm never worked beyond toy models. \n",
    "\n",
    "\n",
    "Actually, all model parameters have multifold effect in an RNN, including those that are associated with the $x$-$y$ connections, e.g. $w_{1,1}$ in the above example.  A key difference is that we can sum up the affluence of a change of $w_{1,1}$ OVER (parallelly) all time steps to conclude the total contribution of that change to the final $y_1^{(10)}$. But for $u_{1,1}$, its effects must be considered THROUGH (consequentially) all time steps.\n",
    "In the limited steps shown in our figure, when a change $\\Delta$ of $u_{1,1}$ happens, $y_1^{(8)}$ will change accordingly. In turn, the change of $y_1^{(8)}$ will again multiply with $\\Delta$ and affect $y_1^{(9)}$. The cumulative multiplicative effect carries on through the time steps as shown in the next figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ref/rnn4.png\" width=\"360px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the $y^{(7)}_1$ is only the first visible output variable in the figure, not the start of the entire computation. \n",
    "\n",
    "It is not difficult to see that the gradient computation of those \"temporal connection weights\" will either over- or underflow in practical computers. To alleviate the impact of consecutive multiplication, people introduced some modulation techniques on the direct effect of $y^{(t-1)}$ on $y^{t}$.  Two representative techniques are the \"gated recurrent unit\" (GRU) and \"long-short-term memory\" (LSTM). \n",
    "\n",
    "Briefly, those modified recurrent units are compound ones, which include multiple types of sub-units. Besides the output variable $y$, there are also auxiliary units that control the \"green light\" for the information from an early moment to proceed to affect the results of a later moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ref/rnn5.png\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above sketches a GRU network. Please refer to the further read section for more information on practical RNNs. The implementation of RNN in `pytorch` is straightforward. E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "rnn = nn.GRUCell(input_size=16, hidden_size=8)\n",
    "# a GRU cell, where the a_t has dimension of 8 and input x_t of 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(6, 1, 16) # batch of 1, 6 time steps\n",
    "h = torch.randn(1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we can perform the calculation.\n",
    "output = []\n",
    "for i in range(6):\n",
    "    h = rnn(x[i], h)\n",
    "    output.append(h)\n",
    "    print(\"Time step {}, input size {}, input-hidden size {}, output {}\"\n",
    "          .format(i, x[i].shape, h.shape, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: the first dimension of `x` now NOT batch samples, but time-steps. The second dimension represents the batch samples, for which we set just to 1 -- meaning we are dealing with one sequence a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative, all time steps can be done using GRU (no \"Cell\" in the class name)\n",
    "torch.manual_seed(1)\n",
    "rnn = nn.GRU(input_size=16, hidden_size=8)\n",
    "x = torch.randn(6, 1, 16)\n",
    "h = torch.randn(1, 1, 8)\n",
    "all_outputs, last_output = rnn(x, h)\n",
    "\n",
    "# Please compare the result to the above. Note the random-seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">__brief topic__</span>\n",
    "\n",
    "This technique is to incorporate the units in lower stages (early computation steps) into the final output in a more direct route. The rationale behind this idea is that the error messages (review the backpropagation part) can be more readily passed to those units and facilitates the training of the deep neural network.\n",
    "\n",
    "The implementation of the technic is straightforward -- just add the input $x$ to the activation of a later layer. Of course, this design introduces a new restriction that the number of units in the layer to which we \"short-cut\" the $x$ from a lower layer must remain the same as $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple implementation of \n",
    "# x -> linear(x) -> y\n",
    "#   |_______________^\n",
    "#    short-cut link\n",
    "\n",
    "x = torch.randn(10, 5)\n",
    "linear = nn.Linear(5, 5)\n",
    "y = nn.functional.relu(linear(x)) + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Injecting noises: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another often adopted simple yet effective technique during __training__ a neural network is _dropout_. Simply put, to perform the computations for a layer with the dropout mechanism, we randomly set all its input variables to zero.\n",
    "\n",
    "The simple technique does not make much sense at first glance. However, the random removal of the input variables implicitly employs an exponentially large ensemble. Examine the figure (excerpt from the original paper) below. \n",
    "\n",
    "<img src=\"ref/dropout.png\" width=\"400px\">\n",
    "\n",
    "In each training iteration, we use one random sparse network out of exponentially large amount of possibilities. All the networks share the weights. At the test stage, the dropout operation is deactivated. We are effectively using the average of all the trained sparse nets for the prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of dropout in `pytorch` is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearD, self).__init__()\n",
    "        self.linear = nn.Linear(5, 3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "lin_mod = LinearD()\n",
    "x = torch.randn(2, 5)\n",
    "\n",
    "# fix a random seed so we can repeat which input\n",
    "# variables are dropped.\n",
    "torch.manual_seed(42)\n",
    "print(\"Train Seed 42\", lin_mod(x))\n",
    "\n",
    "# When put the model into evaluation (test) mode\n",
    "# dropout layer stops working\n",
    "lin_mod.eval()\n",
    "torch.manual_seed(42)\n",
    "print(\"Eval Seed 42\", lin_mod(x))\n",
    "torch.manual_seed(52)\n",
    "print(\"Eval Seed 52\", lin_mod(x))\n",
    "\n",
    "# Set back to train, and we can reproduce the original\n",
    "# dropout result.\n",
    "lin_mod.train()\n",
    "torch.manual_seed(42)\n",
    "print(\"Train Seed 42\", lin_mod(x))\n",
    "# And the network's output is affected by the randomly\n",
    "# dropped variables. Note that we had not changed the\n",
    "# network weights.\n",
    "torch.manual_seed(52)\n",
    "print(\"Train Seed 52\", lin_mod(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with data distribution shifting among layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is well known that most data model relies on some basic implicit assumption about the statistics of the input it expects to accept. For example, if a predictor variable $X$ has mean value of 0.5 and standard variance of 2.0 during the construction of a data model, while during testing, the variable's statistics changed to mean=5.5 and variance=20.0, it is unlikely the model can take good use of $X$ due to the shift of its range.\n",
    "\n",
    "As to deep neural networks, if we view the bottom layers as data feeder to the layers above, the higher-level layers would face the same data-shifting problem as above. More specifically, consider a layer A which is followed by a layer B. When the parameters of A is updated during training, consequently, the statistics of its output change accordingly. This can cause instability for the training of layer B.\n",
    "\n",
    "So a remedy is to normalise the output of an earlier so the statistics of the outputs, i.e. the inputs to the next layer, are stablised. However, if we forcefully shift the statistics to zero-mean-unit-variance, it would impose too much restriction on the expressive capacity of the layer. Thus two parameters, one for the mean and one for the variance are introduced for each output variable of a \"normalised\" layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inplementation in pytorch is simple.\n",
    "# Please perform further tests using practical data\n",
    "class LinearN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearN, self).__init__()\n",
    "        self.linear = nn.Linear(5, 3)\n",
    "        self.bn = nn.BatchNorm1d(num_features=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bn(self.linear(x))\n",
    "\n",
    "torch.manual_seed(1)    \n",
    "bnl = LinearN()\n",
    "\n",
    "x  = torch.randn(10, 5)\n",
    "x[:, 1] += 0.5 # perturb the mean a bit\n",
    "y = bnl(x)\n",
    "print(\"Result\")\n",
    "print(y)\n",
    "print(\"Mean\")\n",
    "print(y.sum(dim=0))\n",
    "print(\"Var\")\n",
    "print(y.std(dim=0))\n",
    "print(list(bnl.parameters()))\n",
    "# please compare the variance parameter of the batch-norm layer\n",
    "# with the mini-batch output's statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bnl.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiser\n",
    "\n",
    "One important issue we will not cover in this lecture is the optimisation of the network parameters. We had calculated the direction along which to adjust the model parameters. However, the direction remains optimal to approach our criterion in only a very small region. Thus it is a complex research area to design and determine the strategy to apply the adjustments. Some of the strategies are adaptive and varies along with the training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check out the example of a popular optimiser as a concrete toolkit for practical exercises. We write a program of learning the parameters of the convolutional model which classifies hand-written digit images as we had studied above. When creating the optimiser, we provide it the parameters to work on as\n",
    "\n",
    "```python\n",
    "optimiser = Adam(access_to_model_parameters, **options)\n",
    "```\n",
    "One of the most important options is a proportional parameter, which determines the step size of each training iteration. (Please review the section on \"gradient-based optimisation\" in this class). It is set up as 0.001 in this example.\n",
    "\n",
    "In each training step, the optimiser clears all parameter gradients computed in the previous steps by calling `optim.zero_grad()`. After computing the loss (i.e. the main criterion to be _minimised_), the statement `loss.backward()` will populate the `grad` field again. Finally, the parameters are updated using `step()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4)\n",
    "conv_mod = ConvNetwork()\n",
    "optimiser = Adam(conv_mod.parameters(), lr=0.001)\n",
    "\n",
    "for batch_idx, (x, y) in enumerate(data_loader):\n",
    "    optimiser.zero_grad()\n",
    "    pred = conv_mod(x)\n",
    "    loss = nn.functional.nll_loss(pred, y)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    if batch_idx % 500 == 0:\n",
    "        print(\"[iteration {}] Loss is {}\".format(batch_idx, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">__Discussion__</span>\n",
    "Discuss the meaning of the \"loss\" in the outputs of the previous program. How it is connected to the classification performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-home points\n",
    "- Recursive design of the neural networks, bottom-up view and top-down view.\n",
    "- Implementation of a multi-layer network.\n",
    "- What is gradient.\n",
    "- Limitation of gradient-based optimisation.\n",
    "- Back-propagation in training.\n",
    "    \n",
    "## Programming skills\n",
    "- setting up `colab`\n",
    "- torch.nn.Module\n",
    "- super-call in sub-classes\n",
    "- class and function strings\n",
    "- simple optimisation procedure\n",
    "\n",
    "## Further reading\n",
    "- The [original dropout paper][8].\n",
    "- Detailed interpretation on batch normalisation[9]\n",
    "- RNN [in details][7]\n",
    "- RNN [GRU units][5] (and more!)\n",
    "- RNN [Course on Language Processing][4]\n",
    "- RNN [Advanced Tutorial and Application][3]\n",
    "\n",
    "- Currently, there is a trend of replacing recurrent networks using attention mechanism, which takes into account not only the relations between consecutive samples, but arbitrary contextual relation in a sequence. See the [tutorial][6] for the development.\n",
    "\n",
    "- pytorch documentation on optimiser Scheduling and Families\n",
    "\n",
    "## Course Project - 1\n",
    "- Follow [tutorial-1][1] and [tutorial-2][2] to build an image classifier. You may find the skills we will introduce in the next class helpful.\n",
    "\n",
    "- Alternative objective: choose a dataset of your interest, build a classifier to do the prediction task.\n",
    "\n",
    "[1]:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "\n",
    "[2]:https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\n",
    "\n",
    "[3]:https://www.youtube.com/watch?v=6niqTuYFZLQ\n",
    "\n",
    "[4]:https://www.youtube.com/watch?v=Keqep_PKrY8\n",
    "\n",
    "[5]:https://www.coursera.org/lecture/nlp-sequence-models/gated-recurrent-unit-gru-agZiL\n",
    "\n",
    "[6]:https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "[7]:http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "[8]:https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "\n",
    "[9]:http://mlexplained.com/2018/01/10/an-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning-part-1/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
