{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Evaluate a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Why linear models and how linear model works\n",
    "__Let's understand the data first!__\n",
    "\n",
    "<span style=\"color:blue\">__DATA__:$\\mathcal{D}$</span>\n",
    "\n",
    "If we rip off all \"semantics/domain-specific interpretations/conceptual icing-on-the-cake-of-theory\", each object of interest boils down to a bunch of numbers in any analytics on a digital computer ([Cool works](https://www.youtube.com/watch?v=Ecvv-EvOj8M) have revealed evidence supportting the brain works the similar way, too!). \n",
    "\n",
    "We call such a collection of numbers, a _sample_. Each of the number is an _attribute_. Say, each sample contains $p$ attributes.\n",
    "\n",
    "Some alternative names of the concepts implies interesting views of the data. \n",
    "- Alternatively, samples are called data points or sample points. It is not hard to imagine that \"point\" alludes to the geometric understanding of the data as living in a $p$-dimensional space, each sample corresponding to a point in the space. And the point is determined by its coordinates which are just the numbers making that sample. \n",
    "- Note an attribute is more generic than a number in a particular sample -- it refers to that number across all possible samples. We can take an attribute-centric view of the data, where each sample is an instance of the $p$ _random variables_.\n",
    "\n",
    "The two views (geometric and stochastic) of the data are non-exclusive. For example, you can think of the data to be analysed as some cloudy stuff spreading (distribution) across the $p$-dimensional space. While the distribution is both unknown and difficult to describe (even if we think for a while it were known), all the analyser can get her hands on is a number of points scattered within the \"distribution cloud\", and the points are our sample points. Here our adventure begins.\n",
    "\n",
    "\n",
    "__Linear model__\n",
    "\n",
    "If we are going to arrive at any single conclusion out of every data sample, i.e. a set of numbers, what shall we do, in the most straightforward way?\n",
    "\n",
    "__Q1__: You are the manager of a small bank, and now its time to review your customer profiles, e.g. to identify potential risky loans. Each customer is represented by two numbers: annual income and debt. Now your customers are $(95, 500), (100, 60), (50, 700), (102, 1000), (60, 185), (30, 500), (75, 530), (297, 104), (70, 0), (5, 0.5)$. If your job is to identify the most risky customers, what will you do with your numbers?\n",
    "- describe your plan\n",
    "- formulate your plan to deal with the numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[Hint-1]__\n",
    "Running the cell below can help you with more intuitive ideas of the data. You don't need to understand the content here, we will explain later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "customers = np.array([(95, 500), (100, 60), \n",
    "    (50, 700), (102, 1000), (60, 185), (30, 500), \n",
    "    (75, 530), (297, 104), (70, 0), (5, 0.5)])\n",
    "plt.scatter(customers[:,0], customers[:,1])\n",
    "plt.xlabel(\"income\")\n",
    "plt.ylabel(\"debt\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[Hint-2]__ \n",
    "You may want to consider a baseline in specifying the term \"risky\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[Hint-3]__\n",
    "This problem is only half-way data analytics, to be honest. The issue is still on the concept of \"risky\". Can you discuss with this respect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear model -- definition and implementation__\n",
    "\n",
    "Now you may have motivated the design of a linear model by yourself! Just assign a weight to each attributes so they together produce a sigle number expressing some view towards a target of analytics.\n",
    "\n",
    "With such motivation, let's study the linear model's implementation and variants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our linear model. It will work for data with 2 attributes.\n",
    "linear_model_w0 = 0.6  # Note index starts from 0 as a widely accepted convention.\n",
    "linear_model_w1 = 0.8\n",
    "linear_model_b  = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0.7, -0.2]  # A data sample, check the \"List\" section in notebook PyGym\n",
    "y = linear_model_w0 * x[0] + linear_model_w1 * x[1] + linear_model_b\n",
    "\n",
    "print(\"Linear model: x\", x, \"->\", y)\n",
    "\n",
    "# if you get a \n",
    "#    NameError: name 'linear_model_w0' is not defined\n",
    "# check if the above cell, defining those stuff, has been executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Model Family and Learning\n",
    "\n",
    "Well, we can define our linear models. In practice, we actually operates with another concept -- _model family_. Each model is a very particular theory about the relationship between the attributes of samples and the corresponding targets. Such a theory is called a _hypothesis_. However, any particular hypothesis may of little use in practice: say you have a linear model with $w_1=0.1573, w_2=-1.2856$ and $b = -0.0021$ it is hard to imagine the model is optimal for any practical 2D data analytic task. So in the business of data analytics, we adopt the _learning_ approach -- where instead of focusing on an individual data model, we define a _model family_, which consists of many, often infinitely many, hypothesis. \n",
    "\n",
    "<span style=\"color:blue\">__HYPOTHESIS SPACE__:$\\mathcal{H}$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a template from which we can realise a family of linear models.\n",
    "class LinearModel2D:\n",
    "    def __init__(self, w0, w1, b):\n",
    "        \"\"\"\n",
    "        :param w0, w1: init values of the weights for two attributes.\n",
    "        :param b: init value of the bias\n",
    "        \"\"\"\n",
    "        self.w0 = w0\n",
    "        self.w1 = w1\n",
    "        self.b  = b\n",
    "        \n",
    "    def evaluate(self, x):  \n",
    "        \"\"\"\n",
    "        :param x: a 2D data sample. \n",
    "        NB: As long as x allows you to extract its 2 attributes using \n",
    "        x[0] and x[1] (see code below), it doesn't matter what is its \n",
    "        specific type. \n",
    "        \n",
    "        ** Function arguments are matched by functionality, not\n",
    "        type definition ** I feel it makes life easier to implement\n",
    "        data analytic algorithms. The feature is called \"duck typing\" \n",
    "        (Google the term)\n",
    "        \"\"\"\n",
    "        return self.w0 * x[0] + self.w1 * x[1] + self.b\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        For pretty print: when you print(a), the actual process is print(a.__str__())\n",
    "        \"\"\"\n",
    "        return \"Linear Model 2D: w0={}, w1={}, b={}\".format(self.w0, self.w1, self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make one linear model\n",
    "model_1 = LinearModel2D(0.6, 0.8, 0.5)\n",
    "model_2 = LinearModel2D(0.6, 1.0, 0.5)\n",
    "\n",
    "print(\"Model 1:\")\n",
    "print(x, \":\", model_1.evaluate(x))  # compare with the result above\n",
    "print(\"Model 2:\")\n",
    "print(x, \":\", model_2.evaluate(x))  # compare with the result above\n",
    "# another data sample\n",
    "x1 = [0.7, 0]\n",
    "print(x1, \":\", model_1.evaluate(x1))  # compare with the result above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q2__ Try another linear model by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Targets__\n",
    "\n",
    "Now let re-think the Hint-3 in __Q1__: the target of analytics should be given for sample examples. This piece of information, or the lack of it, defines many different types of machine learning. If you think about the situation carefully, there is no essential distinction between \"targets\" or \"attributes\" from the point of view of an object. The distinction is made on the model learner's side. Targets are a special set of attributes, which are accessible during the stage we adjust our model. Once the model has been fixed and deployed, the \"target\" variables are no longer available to the model, for its namesake, they becomes the \"target\" of the model prediction. \n",
    "\n",
    "From the various ways the targets are given (or missing) arise supervised (we will see shortly), unsupervised, reinforcement, transfer, ..., learning schemes.\n",
    "\n",
    "Given the hypotheses in some $\\mathcal{H}$, and in the light of data samples drawn from $\\mathcal{D}$, we perform some process to pick up the one that mostly fits. The process is called _learning_.\n",
    "\n",
    "<span style=\"color:blue\">__LEARNING ALGORITHM__:$\\mathcal{A}$</span>\n",
    "\n",
    "The purpose of learning is of course to make the model's prediction on the targets, given the observable attributes, better aligned with that of the true data. Technically, we need a single criterion, you can consider it as the \"KPI\" of the model. We optimise over the model parameters (i.e. picking up a specific model/hypothesis from $\\mathcal{H}$) to have the best KPI-measurement on the _training set_ of data. By convention, the criterion is often formulated as the discrepancy between the desired target and the model prediction, which is to be _minimised_.\n",
    "\n",
    "<span style=\"color:blue\">__LOSS__:$\\mathcal{L}$</span>.\n",
    "\n",
    "By now we have encountered all elements in learning-based machine intelligence $(\\mathcal{D}, \\mathcal{H}, \\mathcal{L}, \\mathcal{A})$ -- if you accept the view of intelligence as \"capable of summarising from past experience to shape behaviour for future reward\". \n",
    "\n",
    "The learning framework is shown in the picture below:\n",
    "![Learning from Data Framework](ref/learning.png)\n",
    "\n",
    "__Q3__\n",
    "Can you identify a key assemption in the entire formulation of the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hint] The learning is on _training set_, while the ultimate goal is the model performance on $\\mathcal{D}$, which is unknown. So we have to hope the training set is representative enough for $\\mathcal{D}$, at least, in terms of the aspects that are concerned by $\\mathcal{L}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A toy example.\n",
    "x_set = [[-0.1, 0.5],\n",
    "         [0.2, 0.1],\n",
    "         [-0.3, -0.3],\n",
    "         [-0.4, 0.4],\n",
    "         [0.1, 0.2],\n",
    "         [-0.5, 0.5]]\n",
    "targets = [+1, +1, -1, -1, +1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_1)\n",
    "for x in x_set:\n",
    "    y = model_1.evaluate(x)\n",
    "    print(x, \":\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Q4__ \n",
    "- Apply another model to the set, see result\n",
    "- Adjust model parameters (so you will have multiple models), check those model’s output on one data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Numpy implementation__\n",
    "\n",
    "Refer to the application of numpy in our PyGym. How can you implement the linear model using numpy? Let us represent sample as $[x_0, x_1, \\dots, x_{d-1}]$, and stack multiple samples, so that each one being a _row_ in a _data matrix_:\n",
    "\n",
    "$\\mathbf{X} = \\left[\\begin{array}{cccc}\n",
    "x_{0,0} & x_{0,1} & \\dots & x_{0,d-1}\\\\\n",
    "x_{1,0} & x_{1,1} & \\dots & x_{1,d-1}\\\\\n",
    " &  & \\dots\\\\\n",
    "x_{n-1,0} & x_{n-1,1} & \\dots & x_{n-1,d-1}\n",
    "\\end{array}\\right]$\n",
    "\n",
    "NB, we use 2 subscriptors for $x_{}$ now, the first one as sample ID, the second one for attributes. I.e. the _second_ subscript is corresponding to the only subscript we used above for a single sample. \n",
    "\n",
    "NB2, in a 2D case, the X-mat is simplified as\n",
    "\n",
    "$\\mathbf{X} = \\left[\\begin{array}{cc}\n",
    "x_{0,0} & x_{0,1}\\\\\n",
    "x_{1,0} & x_{1,1}\\\\\n",
    " \\dots\\\\\n",
    "x_{n-1,0} & x_{n-1,1}\n",
    "\\end{array}\\right]$\n",
    "\n",
    "\n",
    "Now, let us have our $\\boldsymbol{w} = [w_0, w_1]$, forget about the bias $b$ for the moment, the linear model evaluation on each of the samples, i.e. each row of the matrix $\\mathbf{X}$, is\n",
    "\n",
    "$\\begin{array}{c}\n",
    "y_0 = x_{0,0}w_{0}+x_{0,1}w_{1}+\\dots+x_{0,d-1}w_{d-1}\\\\\n",
    "y_1 = x_{1,0}w_{0}+x_{1,1}w_{1}+\\dots+x_{1,d-1}w_{1,d-1}\\\\\n",
    "\\dots\\\\\n",
    "y_n-1 = x_{n-1,0}w_{0}+x_{n-1,1}w_{1}+\\dots+x_{n-1,d-1}w_{d-1}\n",
    "\\end{array}$\n",
    "\n",
    "NB2a: replace $d$ with 2 and re-write the equation yourself, see how it works for a simple 2D case. (You can simply double-click here to edit this cell, and copy-and-paste the above equation below, deleting what is not needed when $d=2$, then pretty-display this cell again by executing it).\n",
    "\n",
    "The above computation is exactly a matrix-by-vector multiplication $\\boldsymbol{y}=\\mathbf{X} \\cdot \\boldsymbol{w}$, and is straightforwardly corresponding to numpy array operation [dot](http://numpy-dot-documnet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q5__ *\n",
    "\n",
    "Perform matrix computation and link to perceptron training\n",
    "Please try to compute:\n",
    "```\n",
    "[[-0.1,  0.5, 1.0],           [0.6,\n",
    " [ 0.2,  0.1, 1.0],    *       0.8,\n",
    " [-0.3, -0.3, 1.0],            0.5]\n",
    " [-0.4,  0.4, 1.0]]  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hint: `[ 0.84  0.7   0.08  0.58]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class LinearModel2D:\n",
    "    def __init__(self, w0, w1, b):\n",
    "        \"\"\"\n",
    "        w-parameter is now stored as a 1-D array.\n",
    "        \"\"\"\n",
    "        self.w = np.asarray([w0, w1])\n",
    "        self.b = b\n",
    "        \n",
    "    def evaluate(self, x):  \n",
    "        \"\"\"\n",
    "        :param x: one or more 2D data sample. \n",
    "        \"\"\"\n",
    "        x = np.atleast_2d(x)  # if you don't do this, you may have \n",
    "        # get error when x is a single-sample dataset. \n",
    "        \n",
    "        # Q: how to implement y[i] = w[0]*x[i, 0] + w[1]*x[i, 1] + b\n",
    "        #    using Python matrix operations?\n",
    "        y = np.dot(x, self.w) # {tutor}\n",
    "        y += self.b           # {tutor} and explain the concept of \"broadcasting\"\n",
    "        return y\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        For pretty print\n",
    "        \"\"\"\n",
    "        return \"Linear Model 2D: w0={}, w1={}, b={}\".format(\n",
    "            self.w[0], self.w[1], self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NB__\n",
    "\n",
    "The mathematical convention is to consider a vector as a _column_ vector. E.g. if $w$ is a 2D vector, we consider $w$ as $\\left[\\begin{array}{c}\n",
    "w_{1} \\\\\n",
    "w_{2}\n",
    "\\end{array}\\right]$, rather than $[w_1, w_2]$.\n",
    "\n",
    "In computer implementation, however, a _vector_ is a complex object has only one dimension, i.e. you traverse the elements in such an object, e.g. `w`, using only _one_ index, `w[0]`, `w[1]`. So technically, this object doesn't have orientation: it depends on the particular package to interpret it as a COLUMN/ROW. If we want to force an interpretation, we may add a _singular dimension_ to the object, a dimension of length 1, thus not adding more contents, but specifying the structure. \n",
    "\n",
    "E.g. in numpy, we can do `w = w[numpy.newaxis, :]` produces a 1-row matrix, i.e. a row vector, and `w = w[:, numpy.newaxis]` results in a column vector (`newaxis` is a special mark in numpy, replace `numpy` with the name given at importing, usually we use `np`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us implement our old friend as an instance of the above defined class\n",
    "lm = LinearModel2D(0.6, 0.8, 0.0)\n",
    "print(lm)\n",
    "\n",
    "# Apply model\n",
    "X = np.asarray(x_set)\n",
    "\n",
    "A = lm.evaluate(X)\n",
    "preds = np.sign(A)\n",
    "for x_, a_, p_, y_ in zip(X, A, preds, targets):  # cf \"zipping\" in PyGym if necessary\n",
    "    print (\"x {}: lm(x) {}: pred {}: ground-truth {}\".format(\n",
    "        x_, a_, p_, y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Learning: Adapting a linear model to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometrical interpretation of linear model *\n",
    "It is straightforward to see that with $p$ data attributes, a linear model represents a slope surface in $(p+1)$-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(x, w, b):\n",
    "    \"\"\"\n",
    "    Solve for y, given x, and line equation\n",
    "    \"\"\"\n",
    "    w1 = w[1] if abs(w[1]) > 1e-9 else 1e-9\n",
    "    return -(b + w[0]*x) / w1\n",
    "\n",
    "x0 = -1.0\n",
    "y0 = get_y(x0, lm.w, lm.b) \n",
    "# Q6: what do we get if we evaluate the model at (x0, y0)?\n",
    "#    verify your solution \n",
    "x1 = 1.0\n",
    "y1 = get_y(x1, lm.w, lm.b)\n",
    "\n",
    "plt.plot([x0, x1], [y0, y1]) # link two points (x0, y0) and (x1, y1)\n",
    "# Q7: what is the meaning of this line?\n",
    "#    Enable the following block of code and execute again,\n",
    "#    verify your theory (about the meaning of the line)\n",
    "\n",
    "ENABLE_DRAW_SAMPLES = True\n",
    "if ENABLE_DRAW_SAMPLES:\n",
    "    A = lm.evaluate(X)\n",
    "    preds = np.sign(A)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=targets, s=128, cmap='summer')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=preds, s=36, cmap='summer')\n",
    "    plt.grid('on')\n",
    "    plt.xlim([-1, +1])\n",
    "    plt.ylim([-1, +1])\n",
    "    plt.title(\"Green: -1; Yellow: +1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q8__\n",
    "    \n",
    "- What does it mean when the colour is consistent?\n",
    "- What (inner or outer) colour reflects the model prediction?\n",
    "- What colour reflects the desired value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q9__\n",
    "\n",
    "Adjust model and check how it changes the line drawn above (need to re-execute the cell above)\n",
    "\n",
    "Hint: locate a wrongly classified sample using the grid provided.\n",
    "Try to adjust lm.w and go back the above cell to see effect. E.g. `lm.w += -np.asarray([-0.5, 0.5])`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron training algorithm \n",
    "\n",
    "The combination of a linear model family and weight-adjust program (to change prediction on samples where it made mistakes) is called a _perceptron_. The full program is lised below. The central idea is consider the weight $\\boldsymbol{w}$ as a direction in the data space of $\\mathcal{X}$. (Forget the baseline for now.) Ideally, the inner product between the weight and some data point $\\langle \\boldsymbol{w}, \\boldsymbol{x}_i \\rangle$ should be align with the corresponding target value $y_i$. Be specific, \n",
    "- for $y_i = -1$: $\\langle \\boldsymbol{w}, \\boldsymbol{x}_i \\rangle < 0$\n",
    "- for $y_i = +1$: $\\langle \\boldsymbol{w}, \\boldsymbol{x}_i \\rangle > 0$\n",
    "what if the current $\\boldsymbol{w}$ doesn't give the desired output on some $\\boldsymbol{x}_i$? \n",
    "\n",
    "__Q10__ *\n",
    "Can you propose a scheme of adjusting $\\boldsymbol{w}$?\n",
    "_HINT_: the self-inner product $\\langle \\boldsymbol{a}, \\boldsymbol{a} \\rangle$ is always non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sketch of the algorithm\n",
    "```\n",
    "while there is mis-classified data, say x\n",
    "    if x: +1, w(t+1) <- w(t) + x\n",
    "    if x: -1, w(t+1) <- w(t) - x\n",
    "```\n",
    "\n",
    "### Sketch of proof that the algorithm finds the solution when there is one\n",
    "\n",
    "1. Consider such an error free $\\boldsymbol{w}*$\n",
    "2. Consider the minimum “safe margin” among all $\\mathcal{X}$ by $\\boldsymbol{w}^*$, must be some $\\rho>0$.\n",
    "3. The alignment of $\\boldsymbol{w}_t$ with $\\boldsymbol{w}^*$ would be improved more than $\\rho$ in each step, i.e. $$\n",
    "\\langle \\boldsymbol{w}_t, \\boldsymbol{w}^* \\rangle - \\langle \\boldsymbol{w}_{t-1}, \\boldsymbol{w}^* \\rangle > \\rho\n",
    "$$  \n",
    "4. The increase of the length of $\\boldsymbol{w}_t$ is limited by the data point of largest length. \n",
    "5. Combine 3. and 4., one can have the _direction_ of $\\boldsymbol{w}_t$, $\\frac{\\boldsymbol{w}_t}{\\|\\boldsymbol{w}_t\\|_2}$, aligns with $\\boldsymbol{w}^*$ with $t$ increasing\n",
    "6. Therefore $\\boldsymbol{w}_t$ will produce desired symbols on all data points with large $t$.\n",
    "\n",
    "A full proof can be found [online](https://matthewhr.wordpress.com/2014/05/30/perceptron-convergence-theorem/).\n",
    "\n",
    "__Q11__ *\n",
    "Consider what if there is no such perfect data separator $\\boldsymbol{w}^*$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(lm, X, y):\n",
    "    \"\"\"\n",
    "    :param lm: a linear model\n",
    "    :param X: 2D sample set\n",
    "    :param y: ground-truth labels\n",
    "    \"\"\"\n",
    "    more = True\n",
    "    while more:\n",
    "        more = False\n",
    "        pred = np.sign(lm.evaluate(X))\n",
    "        no_fit = np.nonzero(pred!=y)[0] # see PyGym np.nonzeront \n",
    "        if len(no_fit)>0:\n",
    "            x_ = X[no_fit[0]] \n",
    "            # Q: here we take the first mis-classified sample. \n",
    "            #    how to take a random one?\n",
    "            y_ = y[no_fit[0]]\n",
    "            lm.w += x_*y_   \n",
    "            lm.b += y_\n",
    "            more = True\n",
    "            # NB: lm changed here, you don't need to explicitly\n",
    "            #     return and re-assign lm. See PyGym:[Variable scope and \n",
    "            #     mutable arguments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)  # random generator\n",
    "w = rng.rand(2) * 2.0 - 1.0\n",
    "b = rng.rand() * 2.0 - 1.0\n",
    "lm = LinearModel2D(w[0], w[1], b)\n",
    "\n",
    "# Q: Let us train the model lm.\n",
    "train_perceptron(lm, X, targets) # {tutor}\n",
    "A = lm.evaluate(X)\n",
    "preds = np.sign(A)\n",
    "for x_, a_, p_, y_ in zip(X, A, preds, targets):\n",
    "    print ((\"X {}, a {}, predicted as {}, ground-truth {}\".format(\n",
    "        x_, a_, p_, y_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron is the beginning and building block\n",
    "Linear models are surprisingly ubiquitous -- perceptrons can be \"stacked\" together:\n",
    "1. take a number of perceptrons, each takes the input and produce own output for any data $x$\n",
    "2. collect the output of all the perceptrons, and feed into another layer of perceptrons\n",
    "3. if top layer reached, done, goto 1. otherwise.\n",
    "Such a structure is called multi-layer perceptron (MLP). It has another name: deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the training process *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_perceptron_with_drawing(lm, X, y):\n",
    "    \"\"\"\n",
    "    Draw each training step to have a pretty understanding.\n",
    "    Try to learn visualisation functions in this function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_y(x, w, b):\n",
    "        w1 = w[1] if abs(w[1]) > 1e-9 else 1e-9\n",
    "        return -(b + w[0]*x) / w1\n",
    "    \n",
    "    \n",
    "    def draw_model_and_samples(X, targets, lm, rounds):\n",
    "        x0 = -np.abs(X[:, 0]).max()\n",
    "        y0 = get_y(x0, lm.w, lm.b) \n",
    "        x1 = -x0\n",
    "        y1 = get_y(x1, lm.w, lm.b)\n",
    "        A = lm.evaluate(X)\n",
    "        preds = np.sign(A)\n",
    "        plt.clf()\n",
    "        plt.plot([x0, x1], [y0, y1]) \n",
    "        plt.scatter(X[:, 0], X[:, 1], c=targets, s=128, cmap='summer')\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=preds, s=36, cmap='summer')\n",
    "        plt.grid('on')\n",
    "        plt.xlim([x0*1.05, x1*1.05])\n",
    "        plt.ylim([-np.abs(X[:, 1]).max()*1.05, np.abs(X[:, 1]).max()*1.05])\n",
    "        plt.title(\"Round {}: Errors {}: Green: -1; Yellow: +1\".format(\n",
    "                rounds, np.count_nonzero(preds!=targets)))\n",
    "    \n",
    "    \n",
    "    more = True\n",
    "    rounds = 0\n",
    "    draw_model_and_samples(X, y, lm, rounds)\n",
    "    plt.show()\n",
    "    while more:\n",
    "        time.sleep(0.5)\n",
    "        more = False\n",
    "        pred = np.sign(lm.evaluate(X))\n",
    "        no_fit = np.nonzero(pred!=y)[0] # see PyGym np.nonzeront \n",
    "        if len(no_fit)>0:\n",
    "            x_ = X[no_fit[0]] \n",
    "            y_ = y[no_fit[0]]\n",
    "            lm.w += x_*y_   \n",
    "            lm.b += y_\n",
    "            more = True\n",
    "            rounds += 1\n",
    "        draw_model_and_samples(X, y, lm, rounds)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)  # random generator\n",
    "w = rng.rand(2) * 2.0 - 1.0\n",
    "b = rng.rand() * 2.0 - 1.0\n",
    "lm = LinearModel2D(w[0], w[1], b)\n",
    "train_perceptron_with_drawing(lm, X, targets)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
